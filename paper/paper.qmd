---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - Ziheng Zhong
thanks: "Code and data are available at: [https://github.com/iJustinn/Canadian_Grocery_Analysis.git](https://github.com/iJustinn/Canadian_Grocery_Analysis.git)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false
# load library
package_list <- c("tidyverse", "kableExtra", "ggplot2", "dplyr", "here", "knitr", "rstanarm", "modelsummary", "readr", "lme4", "tinytex", "reshape2")

install_and_load <- function(package_list) {
  for (package in package_list) {
    if (!require(package, character.only = TRUE)) {
      install.packages(package)
      library(package, character.only = TRUE)
    }
  }
}

install_and_load(package_list)

# load data
ppu_data <- read_csv(here("data", "02-analysis_data", "ppu_data.csv"), show_col_types = FALSE)
beef_data <- read_csv(here("data", "02-analysis_data", "beef_data.csv"), show_col_types = FALSE)

# load model
beef_model <- readRDS (file = here:: here ("models/beef_model.rds"))
```



# Introduction

Overview paragraph

Estimand paragraph

Results paragraph

Why it matters paragraph

Telegraphing paragraph: The remainder of this paper is structured as follows. @sec-data....



# Data {#sec-data}

This project is motivated and guided by Rohan Alexander and his book [@citeTbook]. Data used in this paper was cleaned, analyzed and modeled with the programming language R [@citeR]. Also with support of additional packages in R: `readr` [@citeReadr], `ggplot2` [@citeGgplot2], `tidyverse` [@citeTidyverse], `dplyr` [@citeDplyr], `here` [@citeHere], `knitr` [@citeKnitr], `kableExtra` [@citeKableExtra], `rstanarm` [@citeRstanarm], `modelsummary` [@citeModelsummary],

## Source

The dataset used for this research was sourced from Hammer, a publicly available repository designed to provide pricing data from various retail chains. The dataset comprises detailed information on product attributes, such as product name, vendor, current price, old price, units available, and additional details like the month of certain price. The dataset's broader context is centered on consumer pricing trends across different vendors, helping to understand how factors like vendor type, previous pricing, and seasonal changes influence current prices. This information is pivotal for examining market behaviors and assessing pricing strategies in retail environments.

The dataset variables include several categorical and numerical components. Key variables such as ‘vendor’, which identifies the retailer (e.g., Walmart, Galleria), and ‘product_name’, which details the item, are crucial for understanding distribution channels. Variables like ‘current_price’ and ‘old_price’ offer insights into pricing dynamics, enabling analyses of price changes over time. Additionally, temporal variables such as ‘month’ allow for the identification of seasonal variations in pricing. Summary statistics, along with graphs for each of these variables, help illustrate their distributions and relationships. Graphs have been included to show the frequency distribution for vendors, trends in price changes, and comparisons between old and current prices. Relationships among variables like ‘vendor’ and ‘current_price’ have been highlighted to give a comprehensive view of the dataset.

There were other potential datasets available for this analysis, such as proprietary retail data sources or other consumer purchasing databases. However, they were not selected due to restrictions on data availability, licensing requirements, or lack of detail in specific product-level attributes. The Hammer dataset was chosen as it provides granular pricing data that is crucial for understanding product-level trends across multiple vendors. The dataset also allowed for the construction of new variables, such as the calculated ‘price_per_unit’, which enabled more meaningful comparisons between products. High-level data cleaning included handling missing values in the ‘old_price’ variable by replacing them with the ‘current_price’, ensuring the analysis was consistent and complete.

## Measurement

The process of measurement in this research involves translating real-world phenomena into quantifiable data entries within the dataset. For instance, consumer purchasing behavior, influenced by factors such as vendor type, pricing history, and seasonal changes, has been captured through variables like ‘vendor’, ‘current_price’, and ‘month’. The ‘vendor’ variable serves as an identifier of the source of the product, which helps to provide insights into how different retail environments may affect pricing strategies. The ‘current_price’ and ‘old_price’ variables reflect pricing trends and allow us to measure the temporal changes in consumer costs, thereby linking economic activities to specific data points.

The temporal dimension, captured through the ‘month’ variable, enables us to measure the impact of seasonal patterns on pricing. Constructed variables such as ‘price_per_unit’ provide a standardized measurement that allows comparisons across different products, accounting for varying package sizes or quantities. This transformation from abstract consumer behaviors and retail dynamics into structured data ensures that the analysis remains rooted in real-world market phenomena, thereby enabling a more nuanced understanding of pricing behavior and trends across different vendors.

## Outcome variables
The outcome variable for this study, current_price, serves as the key focus of analysis in understanding product pricing dynamics. This section provides a structured overview of the current_price variable, including its distribution and any observed trends, supported by both summary statistics and visualizations. The aim is to offer a clear understanding of how product prices vary and what factors may influence those variations.

### Current Price
The outcome variable in this study is current_price. This variable represents the price of the product at the time of data collection and is central to understanding the effects of various factors on product pricing. By modeling current_price, we aim to explore how factors such as historical pricing, vendor type, and seasonal influences impact current market prices. This variable is crucial for determining the pricing dynamics employed by different vendors and how those strategies affect consumer costs.

To provide a comprehensive view of the outcome variable, Here's a table for easier understanding (@tbl-current_price). This table provides the minimum, maximum, mean, median, and standard deviation of the current_price variable, offering insights into the overall distribution and variability in product prices.

```{r}
#| label: tbl-current_price
#| tbl-cap: "Summary statistics for the outcome variable (current_price)"
#| echo: false
#| warning: false
#| message: false

outcome_summary <- beef_data %>%
  summarise(
    Statistic = c("Min", "Max", "Mean", "Median", "Standard Deviation"),
    Value = c(
      min(current_price, na.rm = TRUE),
      max(current_price, na.rm = TRUE),
      mean(current_price, na.rm = TRUE),
      median(current_price, na.rm = TRUE),
      sd(current_price, na.rm = TRUE)
    )
  )

outcome_summary %>%
  kable(
    col.names = c("Statistic", "Value"),
    booktabs = TRUE
  )
```

In addition to the summary table, the distribution of current_price is visualized through a histogram (@fig-current_price), highlighting common price ranges and the overall spread. This visualization helps to identify any skewness or clustering in the data, which may indicate specific pricing patterns. Together, these tables and graphs provide a comprehensive understanding of the outcome variable, offering a detailed view of how product prices vary across different vendors and over time.

```{r}
#| label: fig-current_price
#| fig-cap: "Boxplot of the distribution of current prices"
#| echo: false
#| warning: false
#| message: false

ggplot(beef_data, aes(y = current_price)) +
  geom_boxplot(fill = "lightgrey", color = "darkgrey", alpha = 0.7) +
  labs(title = "Boxplot of the Distribution of Current Prices", y = "Current Price (in $)") +
  theme_minimal()

```

The boxplot (@fig-current_price) presented above provides a detailed summary of the distribution of current_price for the products in the dataset. Boxplots are useful visualization tools for understanding the spread, central tendency, and overall variability of continuous data. In this case, the boxplot helps in identifying key features such as the median price, the interquartile range (IQR), and any potential outliers that may exist in the pricing data.

The median, represented by the horizontal line within the box, provides an indication of the central price of products in the dataset, highlighting where the bulk of prices are situated. The box represents the IQR, showing where the middle 50% of data points lie, thus giving insights into price concentration. The whiskers, extending from the box, represent the range within which most prices fall, excluding any extreme values or outliers. No outliers were visibly identified in this boxplot, which suggests that most product prices are consistently within a particular range.

The flat shape of the boxplot with a wide range of the IQR indicates low variability in current product prices. The relatively small difference between the lower and upper bounds of the box implies that the pricing of the products is consistent, with most prices concentrated around the median. This consistency may suggest that vendors are using similar pricing strategies or that there is limited differentiation in the types of products sold across vendors. Overall, the boxplot is a valuable tool for summarizing and communicating the underlying distribution of current_price, which aids in understanding the dynamics of pricing across different vendors.

## Predictor variables
The predictor variables in this study—month, old_price, and vendor—each play an important role in explaining variations in current_price. This section outlines how these variables potentially influence pricing, with a focus on capturing temporal patterns, historical effects, and vendor-specific pricing strategies. The summary tables and visual analyses help contextualize the impact of each predictor on the outcome, providing a broad understanding of the pricing dynamics within the dataset.

### Month
The month variable is used as a predictor to capture potential seasonal influences on pricing. By including month, we can observe if specific times of the year are associated with higher or lower prices. This is particularly important for identifying temporal patterns in pricing, which can be influenced by factors such as holidays, sales events, or seasonal demand changes. Table (@tbl-month_summary) provides a summary of the observations for each month, showing how frequently each month appears in the dataset. This information can help highlight any periods with higher or lower data collection frequency, which could influence the interpretation of seasonal pricing effects.

```{r}
#| label: tbl-month_summary
#| tbl-cap: "Summary statistics for the predictor variable (month)"
#| echo: false
#| warning: false
#| message: false

month_summary <- beef_data %>%
  group_by(month) %>%
  summarise(
    Count = n()
  )

month_summary %>%
  kable(
    col.names = c("Month", "Count"),
    booktabs = TRUE
  )

```

### Old Price
old_price serves as another key predictor variable, representing the historical price of a product. This variable helps gauge the effect of historical pricing on current pricing strategies, indicating whether a product has undergone discounts or price hikes. Table (@tbl-old_price_summary) provides a summary of key statistics for old_price, including minimum, maximum, mean, median, and standard deviation. Understanding the distribution of old_price helps us assess whether historical prices significantly differ from current prices, and whether pricing adjustments (e.g., discounts) have been applied uniformly across products. The relationship between old_price and current_price is visually explored in Chart 4, which helps understand the influence of past pricing decisions on current prices.

```{r}
#| label: tbl-old_price_summary
#| tbl-cap: "Summary statistics for the predictor variable (old_price)"
#| echo: false
#| warning: false
#| message: false

old_price_summary <- beef_data %>%
  summarise(
    Statistic = c("Min", "Max", "Mean", "Median", "Standard Deviation"),
    Value = c(
      Min = min(old_price, na.rm = TRUE),
      Max = max(old_price, na.rm = TRUE),
      Mean = mean(old_price, na.rm = TRUE),
      Median = median(old_price, na.rm = TRUE),
      `Standard Deviation` = sd(old_price, na.rm = TRUE)
  )
)

old_price_summary %>%
  kable(
    col.names = c("Statistic", "Value"),
    booktabs = TRUE
  )

```

### Vendor
The vendor variable is an important categorical predictor that differentiates pricing behavior across different retail chains. It allows us to analyze how pricing strategies vary among vendors such as Walmart and Galleria. Table (@tbl-vendor_summary) shows the count of observations for each vendor, allowing us to compare how frequently data was collected for each retail chain. This helps in understanding the representation of each vendor within the dataset and assessing whether certain vendors may have a stronger influence on the overall analysis. Chart 2, which shows the price difference by vendor, provides insights into how different vendors adjust their pricing strategies, offering valuable comparisons among them. Additionally, Chart 3 presents the average current_price over time, broken down by vendor, providing a clearer picture of how vendor-based strategies influence pricing trends.

```{r}
#| label: tbl-vendor_summary
#| tbl-cap: "Count of observations for each vendor"
#| echo: false
#| warning: false
#| message: false

vendor_summary <- beef_data %>%
  group_by(vendor) %>%
  summarise(
    Count = n()
  )

vendor_summary %>%
  kable(
    col.names = c("Vendor", "Count"),
    booktabs = TRUE
  )

```

# Model

Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model overview

To model the current price of beef, $y_i$, at time $i$, we use a Bayesian linear regression model implemented with the `stan_glm` function in the R package `rstanarm`. The response variable, $y_i$, represents the **current price of beef** in dollars. Our predictors include $x_1$, $x_2$, and $x_3$, which represent the **month**, **old price**, and **vendor**, respectively. Each component of the model is defined and justified below.

## Model set-up

Define $y_i$ as the current price of beef at time $i$, measured in dollars. Let $x_1$, $x_2$, and $x_3$ represent the predictors: $x_1$ is the month (categorical variable), $x_2$ is the old price, and $x_3$ is the vendor (categorical variable).

\begin{align}
y_i | \mu_i, \sigma &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i}
\end{align}

In this model:
- $\alpha$ represents the **intercept** term, capturing the baseline price of beef.
- $\beta_1$ corresponds to the **effect of the month** on the current price, allowing us to capture **seasonal variations** that may impact beef pricing. The month is treated as a categorical variable, acknowledging that different months can bring different demand or supply effects due to holidays, weather, or other seasonal factors.
- $\beta_2$ represents the **effect of the old price** on the current price. This allows us to account for price inertia or trends where past pricing influences current pricing.
- $\beta_3$ represents the **effect of the vendor**, also treated as a categorical variable. Different vendors may have distinct pricing strategies, and including vendor as a categorical predictor helps us capture this vendor-specific pricing variation.

We assume normal priors for the model coefficients and intercept, with mean 0 and standard deviation 2.5:

\begin{align}
\alpha &\sim \text{Normal}(0, 2.5) \\
\beta_1 &\sim \text{Normal}(0, 2.5) \\
\beta_2 &\sim \text{Normal}(0, 2.5) \\
\beta_3 &\sim \text{Normal}(0, 2.5)
\end{align}

These priors are chosen to be **weakly informative**, allowing the data to speak for itself while still providing a reasonable range for the coefficients based on prior expectations. Specifically, a standard deviation of 2.5 reflects our expectation that most reasonable effects should fall within a plausible range without being overly restrictive.

For the residual standard deviation, $\sigma$, we assume an **Exponential(1)** prior:

\begin{align}
\sigma &\sim \text{Exponential}(1)
\end{align}

This prior reflects our belief that the standard deviation should be positive and allows flexibility, while preferring smaller values over larger ones, consistent with the expectation of modest variability around the mean prediction.

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

## Model justification

The choice of predictors is well-balanced in terms of complexity and appropriateness for this scenario. By including **month** and **vendor** as categorical variables, we acknowledge the significant impact that both seasonal changes and vendor-specific factors can have on the current price of beef. The inclusion of the **old price** reflects the idea of price inertia, where historical prices are likely to influence current pricing trends.

The model is neither **overly simplistic** nor **unnecessarily complex**: it captures important predictors without adding extraneous complexity that could lead to overfitting. **Month** and **vendor** are treated as categorical variables rather than grouping by more arbitrary categories, which maintains the expressiveness of our model while being parsimonious.

The model is implemented using `stan_glm` from the `rstanarm` package, a high-level interface to Stan for Bayesian modeling. **Stan** provides powerful sampling algorithms, making it an appropriate tool for fitting our Bayesian model efficiently, ensuring **robust convergence** and accurate posterior estimation.

### Model Assumptions and Limitations
- **Linearity**: The model assumes a linear relationship between the predictors and the response variable. This may be a limitation if the true relationship is nonlinear.
- **Normality of Residuals**: The residuals are assumed to follow a normal distribution. If this assumption does not hold, it could bias the model predictions.
- **Priors**: We used weakly informative priors to allow flexibility. However, if we had prior knowledge of specific pricing patterns, more informative priors could be used for better inference.

### Model Validation and Diagnostics
- **Posterior Predictive Checks**: We performed posterior predictive checks to verify that the model adequately captures the variation in the data. These checks involve comparing simulated data from the posterior distribution to the observed data to check for any discrepancies.
- **Convergence**: Model convergence was assessed using **trace plots** and the **R-hat statistic**, with all values being close to 1, indicating successful convergence.
- **Alternative Models**: We also considered fitting simpler linear models that did not include the vendor or seasonal components. These models showed significantly lower predictive performance, indicating the importance of including both the vendor and seasonal factors for accuracy.

In summary, the chosen model provides a comprehensive yet interpretable understanding of how different factors influence the current price of beef. The Bayesian approach allows us to incorporate uncertainty and prior beliefs, while our specific choice of priors and predictors ensures that the model is well-suited for the available data and research questions.



# Results

## Charts

```{r}
#| label: fig-current_price_distribution
#| fig-cap: "Distribution of Current Prices"
#| echo: false
#| warning: false
#| message: false

ggplot(beef_data, aes(x = current_price)) +
  geom_histogram(binwidth = 1, fill = "lightgrey", color = "darkgrey", alpha = 0.7) +
  geom_density(aes(y = ..count..), color = "red", size = 1) +
  labs(x = "Current Price (in $)", y = "Frequency") +
  theme_minimal()

```

Figure 2 (@fig-current_price_distribution) presents the distribution of current beef prices in the dataset, offering insights into the range and frequency of beef pricing. The horizontal axis represents the price in dollars, ranging from 0 to approximately 25, while the vertical axis indicates the frequency, or the number of observations, within each price range. The grey bars illustrate the frequency distribution, showing how beef prices are spread across different price levels.

The histogram indicates that the most frequent price range is between \$5 and \$10, with a notable peak in the frequency around \$7 to \$8. This suggests that a significant proportion of beef products are priced within this range, making it a central pricing cluster in the market. Additionally, there are smaller peaks around \$13 and \$15, hinting at secondary groupings where beef prices are relatively common. The declining frequency beyond \$15 suggests fewer beef products are priced in the higher range, indicating that higher prices are less typical in this dataset.

The overlaid red line represents a smoothed density curve that helps to visualize the general shape and distribution trend of the prices. The curve follows the histogram closely, showing a skewed distribution with a concentration towards the lower prices, and tails off as prices increase. This pattern suggests that while lower-priced beef products are more common, higher prices are less frequent, potentially indicating a general preference or affordability constraint for lower-cost beef among consumers. The presence of multiple peaks could also imply different pricing tiers, perhaps influenced by factors such as beef quality or specific product types.

```{r}
#| label: fig-price_difference_by_vendor
#| fig-cap: "Price Difference by Vendor"
#| echo: false
#| warning: false
#| message: false

beef_data <- beef_data %>%
  mutate(price_difference = old_price - current_price)

ggplot(beef_data, aes(x = vendor, y = price_difference)) +
  geom_boxplot(fill = "lightgrey", color = "darkgrey", alpha = 0.7) +
  labs(x = "Vendor", y = "Price Difference (in $)") +
  theme_minimal()

```

Figure 3 (@fig-price_difference_by_vendor) illustrates the distribution of price differences between beef products offered by two vendors, T&T and Walmart, providing an insight into the variability of pricing across these retailers. The y-axis represents the price difference in dollars, while the x-axis labels the two vendors under comparison. The boxplots visually depict the range, median, and variability in the price differences for each vendor.

For T&T, the boxplot shows a wider range of price differences, with the interquartile range (IQR) extending from roughly \$1.5 to \$4, indicating a moderate to substantial variability in price differences among the products they sell. The median price difference is around \$2.5, suggesting that many of the products have a price difference near this value. However, T&T also shows significant variability with the presence of two outliers beyond \$7.5 and \$10, indicating that a few beef products exhibit much larger price differences compared to the rest of the data.

In contrast, Walmart exhibits a narrower spread of price differences, as indicated by the more compact boxplot. The IQR for Walmart lies approximately between \$1 and \$2.5, and the median price difference is lower compared to T&T, close to \$1.5. This suggests that Walmart’s beef products tend to have smaller and less variable price differences, indicating more consistent pricing practices. There are also a few outliers above \$4, but these are less extreme compared to T&T's outliers.

Overall, the boxplots suggest that T&T has more variability in their pricing, possibly reflecting differences in product types or marketing strategies, whereas Walmart tends to maintain a narrower price range with fewer significant deviations. This disparity may indicate that Walmart focuses on more standardized pricing practices, which could influence consumer choice based on price predictability and consistency.

```{r}
#| label: fig-average_price_over_time
#| fig-cap: "Average Current Price Over Time"
#| echo: false
#| warning: false
#| message: false

beef_data <- beef_data %>%
  mutate(date = as.Date(paste(year, month, day, sep = "-"), format = "%Y-%m-%d"))

avg_price_per_date <- beef_data %>%
  group_by(date) %>%
  summarise(avg_current_price = mean(current_price, na.rm = TRUE))

ggplot(avg_price_per_date, aes(x = date, y = avg_current_price)) +
  geom_line(color = "darkgrey", size = 1) +
  labs(x = "Date", y = "Average Current Price (in $)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Figure 4 (@fig-average_price_over_time) shows the trend of average beef prices over time, providing insight into fluctuations in the market from July to November. The y-axis represents the average current price in \$, while the x-axis represents the date, with labels indicating the progression through the months.

The line chart indicates that average beef prices experienced significant variability throughout the observed period, with several sharp peaks and troughs. In early July, prices were relatively high, reaching a peak of approximately \$13, but this was followed by a rapid decline, dropping to around \$7 by the end of the month. The fluctuations continue through August, where we see notable volatility, including several spikes back to higher average prices of over \$10. Such fluctuations suggest sensitivity in pricing, possibly influenced by seasonal changes, supply chain disruptions, or market demand dynamics.

Entering September and October, there is a pronounced drop in prices, with average prices occasionally dipping below \$6, reflecting a period of reduced pricing. Following this low, prices exhibit recovery and some stabilization, hovering around \$8 to \$9 from October into early November, albeit with some ongoing small fluctuations. The chart’s volatility indicates that beef prices are influenced by various factors throughout these months, with possible implications from supply conditions, retailer pricing strategies, or changing consumer demand. This temporal analysis of beef pricing is critical for understanding market stability and identifying periods of significant price changes that may warrant further investigation.

```{r}
#| label: fig-current_vs_old_price
#| fig-cap: "Heatmap of Price Levels between Current Price and Old Price of Beef"
#| echo: false
#| warning: false
#| message: false

# Create price levels for old_price and current_price with finer granularity
beef_data <- beef_data %>%
  mutate(
    old_price_level = cut(old_price, breaks = seq(0, max(old_price, na.rm = TRUE), by = 2), include.lowest = TRUE),
    current_price_level = cut(current_price, breaks = seq(0, max(current_price, na.rm = TRUE), by = 2), include.lowest = TRUE)
  )

# Count occurrences of price levels
price_level_counts <- beef_data %>%
  count(old_price_level, current_price_level, name = "count")

# Create the heatmap
ggplot(data = price_level_counts, aes(x = old_price_level, y = current_price_level, fill = count)) +
  geom_tile(color = "lightgrey") +
  scale_fill_gradient(low = "lightgrey", high = "red", name = "Frequency") +
  labs( x = "Old Price Level", y = "Current Price Level") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```

Figure 5 (@fig-current_vs_old_price) has been modified to display a heatmap of the relationship between old and current price levels for beef, providing a more aggregated view of how pricing has evolved over time for various products. The x-axis represents the old price level, while the y-axis indicates the current price level, with each cell in the heatmap representing a specific combination of these price levels. The color intensity, ranging from light red to dark red, corresponds to the frequency of observations, as indicated by the legend on the right.

The heatmap reveals a clear diagonal pattern where higher old price levels correspond to higher current price levels, indicating a strong positive association between historical and current pricing. The darker cells along the diagonal reflect the highest frequency of observations, suggesting that most products maintain similar relative pricing over time, reinforcing the consistency in price adjustments observed earlier. This alignment along the diagonal suggests that beef pricing adjustments have been proportional and systematic, potentially influenced by inflation, consistent vendor practices, or other market mechanisms.

The off-diagonal cells that are lightly shaded or empty indicate fewer occurrences where current prices deviate significantly from historical ones. The absence of significant variation off the main diagonal suggests that the majority of pricing changes for beef are directly related to past prices, implying stability in pricing trends with limited irregular adjustments. This consistency may be indicative of structured pricing strategies across different market conditions, with only minor deviations that could result from specific product promotions or temporary supply issues.

Overall, the heatmap effectively illustrates the overall pattern of pricing consistency, highlighting that both old and current beef prices are closely aligned. This representation captures the density of occurrences more clearly than individual scatter points, providing a visual summary of the frequency and typicality of price pairings between historical and current beef prices.

```{r}
#| label: fig-unit_price_distribution
#| fig-cap: "Distribution of Unit Prices"
#| echo: false
#| warning: false
#| message: false

ggplot(beef_data, aes(x = price_per_unit)) +
  geom_histogram(binwidth = 0.1, fill = "lightgrey", color = "darkgrey", alpha = 0.7) +
  geom_density(aes(y = after_stat(count)), color = "red", linewidth = 1) +
  labs(x = "Unit Price ($ per 100 grams)", y = "Frequency") +
  theme_minimal()

```

Figure 6 (@fig-unit_price_distribution) illustrates the distribution of unit prices for beef, measured in \$ per 100 grams. The x-axis represents the unit price, while the y-axis indicates the frequency of occurrences for each price range. The histogram, represented by grey bars, shows how often each unit price appears in the dataset, and the red curve provides a smoothed density estimate to visualize the general trend.

The most striking feature of this figure is the concentration of unit prices near zero, with a significant spike in the frequency at this price point. This suggests that a substantial number of observations in the dataset have a very low or nominal unit price, which could indicate promotions, discounted products, or even data entry inconsistencies that require further examination. After this initial peak, the frequency drops sharply, followed by a few smaller peaks across higher unit price ranges, particularly around \$1, \$3, and \$6 per 100 grams. These smaller peaks indicate the presence of additional groupings of unit prices, potentially corresponding to different product categories, quality grades, or pricing tiers.

The overall distribution is heavily right-skewed, suggesting that the majority of beef products have lower unit prices, while higher prices are less common and scattered. This right-skewness might be indicative of pricing strategies that cater to a wide consumer base, where most products are priced affordably, with only a few premium items available at significantly higher unit prices. The presence of several distinct peaks, rather than a single uniform distribution, suggests price differentiation that could be driven by various factors, such as beef cut type, quality, or retailer-specific pricing strategies. This distribution pattern highlights the diverse pricing landscape for beef products in the dataset, suggesting the existence of both low-cost and premium segments in the market.



## Model

```{r}
#| label: tbl-modelresults
#| tbl-cap: ""
#| echo: false
#| warning: false
#| message: false

modelsummary::modelsummary(
  list(
    "Beef model (Bayesian)" = beef_model
  ),
  statistic = "mad",
  fmt = 2
)

```

Table 5 (@tbl-modelresults) summarizes the results from the Bayesian model applied to analyze beef prices, including key parameter estimates and model diagnostics. The model provides insight into the relationship between the current price of beef and various predictors such as the old price, vendor, and the time of observation (represented by the "month" variable).

The estimated intercept of -0.37 suggests that, after controlling for the predictors in the model, the baseline level of the current price is negative, though this value must be interpreted with caution in terms of its contextual implications. The "month" coefficient is -0.01, indicating a slight negative association between the time variable and current price, although the magnitude of the effect is relatively small. This suggests that as time progresses, there is a minor decrease in the current price, potentially reflecting seasonal effects or gradual changes in market dynamics. The "old_price" coefficient is 0.81, showing a strong positive relationship with the current price, meaning that for each unit increase in the old price, the current price increases by a comparable margin. This result implies a consistent upward adjustment in beef pricing over time, reinforcing the trends seen in previous charts. Finally, the "vendorWalmart" coefficient is 0.56, indicating that beef sold by Walmart tends to be priced higher, on average, than that sold by the baseline vendor (T&T), which aligns with our earlier findings on vendor price differences.

In terms of model fit, the R-squared (R²) value of 0.945 and the adjusted R-squared value of 0.944 indicate a high degree of explained variance, suggesting that the model is capable of capturing the main factors influencing beef prices. The negative log-likelihood value (-4349.672) and expected log pointwise predictive density (ELPD) of -4354.7 provide additional metrics on model fit, while other diagnostic values, such as the leave-one-out information criterion (LOOIC) and Widely Applicable Information Criterion (WAIC), both around 8709, provide an indication of model performance for out-of-sample predictive accuracy. The root mean square error (RMSE) is 1.02, suggesting the model's predictions have a relatively small average deviation from the actual values, indicative of good predictive performance.

These results together illustrate the strong relationship between past pricing and current pricing, as well as vendor influence on beef pricing. The Bayesian approach has provided robust parameter estimates while accounting for uncertainty, as reflected in the provided standard errors. Overall, the model appears to perform well, with high explained variance and reasonable prediction accuracy, making it a valuable tool for understanding and forecasting beef price trends.

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}

# Additional data details

## Vendor Choice
The selection of Walmart and T&T as representative vendors for this analysis was driven by their distinctive characteristics and market coverage, which together offer a comprehensive view of beef pricing dynamics. Walmart is a prominent multinational retailer with a broad presence across North America, making it a well-suited representative of mainstream, large-scale grocery retailing. Its extensive reach and emphasis on standardized pricing provide valuable insights into the general market trends and pricing behaviors that are accessible to a wide segment of consumers. By including Walmart in the analysis, we can capture a perspective that is reflective of the typical shopping experience for many consumers, characterized by competitive pricing and a broad product selection.

On the other hand, T&T Supermarket represents a niche segment of the market, catering specifically to Asian communities and consumers seeking specialty goods that align with Asian culinary traditions. T&T's focus on products tailored to Asian tastes, along with its unique supply chains and vendor relationships, provides an important contrast to Walmart. The inclusion of T&T allows for an exploration of how cultural preferences and niche market positioning influence pricing. By analyzing T&T, we gain insights into pricing practices that reflect the demands of a distinct consumer group and how these differ from the broader market.

The combination of Walmart and T&T enables the analysis to explore both mainstream and culturally specific market behaviors. This choice of vendors thus enriches the study by accounting for variations in consumer preferences, pricing strategies, and market positioning. Walmart's emphasis on scale and cost-efficiency contrasts with T&T's specialization and cultural targeting, allowing us to draw meaningful comparisons in terms of beef pricing strategies between a general and a specialized market context. Such a nuanced understanding is essential for capturing the diversity within the beef retail market, providing a more holistic view of the factors driving beef prices in different retail settings.

## Price per unit

```{r}
#| label: tbl-raw_price_per_unit
#| tbl-cap: "Price per Unit Data for TandT and Walmart"
#| tbl-subcap: ["Raw Data", "Cleaned Data"]
#| echo: false
#| warning: false
#| message: false
#| layout-ncol: 2


ppu_data %>%
  filter(vendor == "TandT") %>%
  head(6) %>%
  bind_rows(
    ppu_data %>%
      filter(vendor == "Walmart") %>%
      head(6)
  ) %>%
  kable(
    col.names = c("Vendor", "Price per Unit"),
    booktabs = TRUE
  )

beef_data %>%
  filter(vendor %in% c("TandT", "Walmart")) %>%
  select(vendor, price_per_unit) %>%
  group_by(vendor) %>%
  summarise(price_per_unit = list(head(price_per_unit))) %>%
  tidyr::unnest(price_per_unit) %>%
  kable(
    col.names = c("Vendor", "Price per Unit"),
    booktabs = TRUE
  )

```

In cleaning the `price_per_unit` variable, I began by extracting the numerical value using `str_extract()`, which allowed me to identify the dollar amounts in the original string (`\\$[0-9\\.]+`), as shown in @tbl-price_per_unit-1. This step ensured that only valid price values, starting with a dollar sign, were captured, removing any extraneous text or symbols.

Next, I removed the dollar sign itself with `str_remove("\\$")` to convert the extracted value into a format suitable for numerical operations. After this, I used `as.numeric()` to convert the string into a numeric type, allowing for further quantitative analysis.

To handle missing values, I applied `ifelse(is.na(price_per_unit), 0, price_per_unit)`, which replaced any `NA` values in the `price_per_unit` column with `0`. This step ensured that all rows had a value for `price_per_unit`, preventing issues during subsequent analysis. Using zero as the replacement helped maintain continuity in the dataset, although I acknowledge that this choice could imply that missing values are negligible, which might require additional context or consideration during interpretation. The price_per_unit column in the dataset after cleaning looks like @tbl-price_per_unit-2.

# Model details {#sec-model-details}

## Posterior predictive check
In @fig-ppcheckandposteriorvsprior-1, we implement a posterior predictive check. This plot shows the observed data distribution (in dark lines) compared with simulated datasets from the posterior (in light grey lines). The purpose of this check is to evaluate how well the model can reproduce the observed data. The close alignment between the simulated posterior predictive distributions and the observed data suggests that the model is accurately capturing the underlying patterns in the data. Specifically, we observe that the peaks and troughs in the observed data are well represented by the posterior samples, indicating that the model is capable of capturing key features such as the general shape and spread of the data distribution. The variability across the posterior predictive samples is consistent with the observed variability, suggesting no significant model misfit.

In @fig-ppcheckandposteriorvsprior-2, we compare the posterior with the prior distributions for each parameter. The posterior distribution (shown on the left) is more concentrated compared to the prior (shown on the right), which demonstrates how the data has influenced and updated our beliefs about the model parameters. For instance, the posterior distribution for the intercept has shrunk considerably compared to its prior, indicating that the data has substantially informed our estimate of the baseline price level. Similarly, for the other parameters (month, old_price, sigma, and vendorWalmart), the posteriors are less dispersed than the priors, suggesting a meaningful reduction in uncertainty. Notably, the posterior intervals are much narrower, especially for the intercept and old price parameters, indicating that the data has provided strong information about these effects. The results demonstrate that the priors were sufficiently weak, allowing the data to dominate the inference and significantly refine the parameter estimates.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

# Posterior predictive check
pp_check(beef_model, plotfun = "dens_overlay")

# Comparing posterior with prior
posterior_vs_prior(beef_model, color_by = "parameter") + theme_minimal() + scale_fill_grey() + scale_color_grey()
```

## Diagnostics
In @fig-stanareyouokay-1, we present a trace plot. This plot shows the sampled values for each parameter across the MCMC iterations. The trace plot helps assess whether the chains have converged to a stationary distribution. In these plots, we see that each parameter's chains are well mixed, with no discernible trends, indicating that the MCMC algorithm has adequately explored the parameter space. The lack of significant upward or downward drift suggests that all chains are consistently sampling from the target distribution, which is a strong indicator of convergence.

Specifically, the chains for each parameter appear to oscillate around a constant mean, and the overlap between the four chains is extensive. This well-mixed appearance across all parameters, including the intercept, month, old price, vendorWalmart, and sigma, gives us confidence that our model has reached convergence, with each chain sampling from similar regions of the posterior distribution.

In @fig-stanareyouokay-2, we present an Rhat plot. The Rhat statistic (also known as the potential scale reduction factor) measures the ratio of the variance between chains to the variance within chains, and values close to 1 indicate that the chains have converged. In this plot, we see that all Rhat values for the parameters are very close to 1, specifically below the threshold of 1.05, suggesting that convergence has been achieved across all parameters.

The fact that all Rhat values are below 1.05 suggests that the different chains are in agreement about the underlying posterior distributions. This indicates that our model's parameter estimates are stable, and the MCMC algorithm has effectively converged. As such, we can be confident in the reliability of the estimates provided by our Bayesian model.

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

# Trace plot
plot(beef_model, plotfun = "trace") + theme_minimal()

# Rhat plot
plot(beef_model, plotfun = "rhat") + theme_minimal()
```



\newpage


# References


